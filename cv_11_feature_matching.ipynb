{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching features\n",
    "This notebook shows how we can match features between frames. This is a core component for many systems. \n",
    "For example when you want to reconstruct a model of the environment, or caluclate how the camera is moving or both at the same time, i.e., simultaneously build a model of the environment and estimating how the camera is moving.. \n",
    "\n",
    "A feature typically consists of a keypoints (location, i.e., where in the image) and a descriptor (what that part of the image looks like). When performing matching one often uses the descriptor first. In ideal cases the descriptor is so good that it can directly generate only correct matches. In most pratcical cases this is not true and we need to look for the correct matches. \n",
    "\n",
    "In the code below we will make use of the ORB feature. ORB is a combination of the FAST detector and the BRIEF descriptor. One of the benefits of ORB is that it is fast to compute and match.\n",
    "https://www.researchgate.net/publication/221111151_ORB_an_efficient_alternative_to_SIFT_or_SURF\n",
    "\n",
    "We make use of David Lowe's ratio test which he defined when he introduced the SIFT feature in https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf. The idea is that we want to only use matches where two points A and B in two images are much better matches than A and the second best match in the second image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Load the image as gray scale images\n",
    "img1 = cv.imread('test_images/patrics_foot_1.jpg',0)\n",
    "img2 = cv.imread('test_images/patrics_foot_2.jpg',0)\n",
    "print(\"The dimension of img1 is \" + repr(img1.shape))\n",
    "print(\"The dimension of img2 is \" + repr(img2.shape))\n",
    "\n",
    "# Initiate a feature detector\n",
    "detector = cv.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors\n",
    "kp1, des1 = detector.detectAndCompute(img1,None)\n",
    "kp2, des2 = detector.detectAndCompute(img2,None)\n",
    "\n",
    "# We use a brute force matcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Apply David Lowe's ratio test to remove bad matches\n",
    "# https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good.append([m])\n",
    "        \n",
    "# Draw the matches, both all of them and the good ones\n",
    "image_matches_all = cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None)\n",
    "image_matches_good = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None)\n",
    "\n",
    "cv.namedWindow(\"All matches between images\", cv.WINDOW_NORMAL)\n",
    "cv.resizeWindow(\"All matches between images\", (1000, 300))\n",
    "cv.namedWindow(\"Good descriptor matches between images\", cv.WINDOW_NORMAL)\n",
    "cv.resizeWindow(\"Good descriptor matches between images\", (1000,380))\n",
    "\n",
    "cv.imshow(\"All matches between images\", image_matches_all)\n",
    "cv.imshow(\"Good descriptor matches between images\", image_matches_good)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
